{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBFqSEkKqpCN"
      },
      "source": [
        "# Lab Deep Learning/ Recurrent Neural Networks/ in pytorch\n",
        "\n",
        "## Using Many-to-One for movie rating predicton\n",
        "\n",
        "**Author: created by geoffroy.peeters@telecom-paris.fr** with the help of Stéphane Lathuilière\n",
        "\n",
        "For any remark or suggestion, please feel free to contact me.\n",
        "\n",
        "## Objective:\n",
        "You will implement two different networks to perform the automatic rating (0 or 1) of movies given the text of their reviews.\n",
        "You will use the ```imdb``` (internet movie database) dataset.\n",
        "\n",
        "The reviews are already available in the form of indexes that point to a word dictionary: each word is already encoded as an index in the dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmkCSNaXLqjh"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AOqjzDwioJj9"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import imdb\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from argparse import Namespace\n",
        "\n",
        "colab = True\n",
        "student = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Yp4OQVvUtr"
      },
      "source": [
        "## Parameters of the model\n",
        "\n",
        "- We only consider the most used words in the word dictionary, we consider the top `param.n_word`\n",
        "- We truncate/zero-pad each review to a length `param.T_x`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4C_Pv7rYvRkM"
      },
      "outputs": [],
      "source": [
        "param = Namespace()\n",
        "\n",
        "param.n_word = 5000 # --- input dimension\n",
        "param.T_x = 100 # --- review length\n",
        "param.index_word_from = 3 # --- indicate where the index start from (first index are used to indicate `PAD` `START` `UNK` tokens)\n",
        "\n",
        "param.n_embedding = 32 # --- dimension of the embedding\n",
        "param.n_lstm = 100 # --- dimension of the LSTM (for a<t> and c<t>)\n",
        "param.n_out = 1 # --- binary classification problem\n",
        "\n",
        "param.batch_size = 64\n",
        "param.lr = 0.001\n",
        "param.n_epoch = 8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsNcRimyLzgP"
      },
      "source": [
        "## Import IMDB data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gfe1ex8oN8Q",
        "outputId": "c75c72e9-9297-4077-e888-9335564f9d84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# --- Import the IMDB data and only consider the ``param.n_word``` most used words\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=param.n_word, index_from=param.index_word_from )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSc5LmksOLyr"
      },
      "source": [
        "## Data content\n",
        "\n",
        "- ```X_train``` and ```X_test``` are each a numpy array, shape=(25000,), of lists.\n",
        "  - Each list represent a review; it is a sequence (represented as a list) of indexes (position of each word in the dictionary)\n",
        "\n",
        "- ```y_train``` and ```y_test``` are each a numpy array, shape=(25000,) of intergers.\n",
        "  - Each integer represent the values 0 (bad movie) or 1 (good movie)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "WouODCPrtiuu",
        "outputId": "48b3da60-048d-4b36-abce-9a0a962cdf0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type(X_train): <class 'numpy.ndarray'>\n",
            "number of training sequences: X_train.shape: (25000,)\n",
            "type(X_train[0]): <class 'list'>\n",
            "length of the first training sequence: len(X_train[0]): 218\n",
            "length of the second training sequence: len(X_train[1]): 189\n",
            "list of data of the first training sequence: X_train[0]: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "maximum length of a training sequence: 2494\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"type(X_train):\", type(X_train))\n",
        "print(\"number of training sequences: X_train.shape:\", X_train.shape)\n",
        "print(\"type(X_train[0]):\", type(X_train[0]))\n",
        "print(\"length of the first training sequence: len(X_train[0]):\",len(X_train[0]))\n",
        "print(\"length of the second training sequence: len(X_train[1]):\",len(X_train[1]))\n",
        "print(\"list of data of the first training sequence: X_train[0]:\", X_train[0] )\n",
        "len_list = [len(train) for train in X_train]\n",
        "print(\"maximum length of a training sequence:\", max(len_list))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(len_list, 100);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I-cEKUh_HM4"
      },
      "source": [
        "## Details of how the reviews are encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcOwiMUT_HM5",
        "outputId": "1b05bf09-c7d2-48fa-f169-5ca2f6eab0c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "<START> although i had seen <UNK> in a theater way back in <UNK> i couldn't remember anything of the plot except for vague images of kurt thomas running and fighting against a backdrop of stone walls and disappointment regarding the ending br br after reading some of the other reviews i picked up a copy of the newly released dvd to once again enter the world of <UNK> br br it turns out this is one of those films produced during the <UNK> that would go directly to video today the film stars <UNK> <UNK> kurt thomas as jonathan <UNK> <UNK> out of the blue to <UNK> the nation of <UNK> to enter and hopefully win the game a <UNK> <UNK> <UNK> by the khan who <UNK> his people by yelling what sounds like <UNK> power the goal of the mission involves the star wars defense system jonathan is trained in the martial arts by princess <UNK> who never speaks or leaves the house once trained tries to blend in with the <UNK> by wearing a bright red <UNK> with <UNK> of blue and white needless to say <UNK> finds himself running and fighting for his life along the stone streets of <UNK> on his way to a date with destiny and the game br br star kurt thomas was ill served by director robert <UNK> who it looks like was never on the set the so called script is just this side of incompetent see other reviews for the many <UNK> throughout the town of <UNK> has a few good moments but is ultimately ruined by bad editing the ending <UNK> still there's the <UNK> of a good action adventure here a hong kong version with more <UNK> action and faster pace might even be pretty good\n"
          ]
        }
      ],
      "source": [
        "word_to_id = imdb.get_word_index()\n",
        "word_to_id = {key:(value+param.index_word_from) for key,value in word_to_id.items()}\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2\n",
        "\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "print(' '.join(id_to_word[id] for id in X_train[1000] ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfl42LGCugWB",
        "outputId": "46469f1e-2a97-46a0-c449-f8b1c4d6ff0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type(y_train): <class 'numpy.ndarray'>\n",
            "y_train.shape: (25000,)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "np.int64(1)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"type(y_train):\", type(y_train))\n",
        "print(\"y_train.shape:\", y_train.shape)\n",
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVw65PNNuobX",
        "outputId": "af61cbae-8367-4fb7-dfc5-d1596e028045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_test.shape: (25000,)\n",
            "y_test.shape: (25000,)\n"
          ]
        }
      ],
      "source": [
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"y_test.shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V18OA7oQNH3c"
      },
      "source": [
        "## Data processing\n",
        "\n",
        "Sequences (represented as a list of values) in `X_train` represent the reviews.\n",
        "They can have different length $T_x$.\n",
        "To train the network we should modify them so that they all have the same length `param.T_x`.\n",
        "\n",
        "We do this by:\n",
        "- **truncating** the ones that are too long,\n",
        "- **padding-with-zeros** the ones that are too short.\n",
        "\n",
        "This can be done at the start of the sequence (`pre`) or at the end (`post`).\n",
        "\n",
        "In our use-case (rating of reviews), the decision ($\\hat{y}$) is taken after reading the whole sentence/review ${x^{<t>}}$. Therefore we will truncate and pad-with-zeroes in `pre` mode (truncate the beginning of the sequence if too long, or add zeroes add the beggining of the sequence if too short)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8UIF_RBUkfQI"
      },
      "outputs": [],
      "source": [
        "def do_pad_sequences(sequences, required_len, truncating='pre', padding='pre'):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "        sequences:  numpy arrays of lists, shape=(25000,)\n",
        "        required_len:     required length of each sequence after truncating and padding\n",
        "        padding     'pre' or 'post' mode\n",
        "        truncating  'pre' or 'post' mode\n",
        "    Returns\n",
        "    -------\n",
        "        padded_sequences    numpy arrays of lists (each list has now length maxlen)\n",
        "    \"\"\"\n",
        "    if student:\n",
        "        # --- START CODE HERE (01)\n",
        "        padded_sequences = []\n",
        "        for seq in sequences:\n",
        "            if len(seq) > required_len:\n",
        "                if truncating == 'pre':\n",
        "                    seq = seq[-required_len:]\n",
        "                else:  # truncating == 'post'\n",
        "                    seq = seq[:required_len]\n",
        "            if len(seq) < required_len:\n",
        "                if padding == 'pre':\n",
        "                    seq = [0] * (required_len - len(seq)) + seq\n",
        "                else:  # padding == 'post'\n",
        "                    seq = seq + [0] * (required_len - len(seq))\n",
        "            padded_sequences.append(seq)\n",
        "        padded_sequences = np.array(padded_sequences)\n",
        "        # --- END CODE HERE\n",
        "    return padded_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhmiHsOGoRwT",
        "outputId": "e48b417a-8e08-47e5-d7a4-ff87b636afbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(X_train[0]): 100\n",
            "len(X_train[1]): 100\n",
            "X_train[0]: [1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
            "    2    8    4  107  117    2   15  256    4    2    7 3766    5  723\n",
            "   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
            "  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
            "    2   18    4  226   22   21  134  476   26  480    5  144   30    2\n",
            "   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
            "   88   12   16  283    5   16 4472  113  103   32   15   16    2   19\n",
            "  178   32]\n"
          ]
        }
      ],
      "source": [
        "# --- truncate and pad input sequences\n",
        "X_train = do_pad_sequences(X_train, required_len=param.T_x, padding='pre', truncating='pre')\n",
        "X_test = do_pad_sequences(X_test, required_len=param.T_x, padding='pre', truncating='pre')\n",
        "\n",
        "print(\"len(X_train[0]):\", len(X_train[0]))\n",
        "print(\"len(X_train[1]):\", len(X_train[1]))\n",
        "print(\"X_train[0]:\", X_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8IktAODF8g9"
      },
      "source": [
        "# Define training and testing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "g45DC3JEF72A"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, data_loader, criterion, optimizer):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    total_loss, total_acc = 0, 0\n",
        "    for X, y in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        hat_y = model(X)\n",
        "        loss = criterion(hat_y.squeeze(), y)\n",
        "        loss.backward() # --- SPECIFIC TO TRAINING\n",
        "        optimizer.step() # --- SPECIFIC TO TRAINING\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = (hat_y.squeeze() > 0.5).float()\n",
        "        total_acc += (predicted == y).sum().item()/len(y)\n",
        "\n",
        "    return total_loss/len(data_loader), total_acc/len(data_loader)\n",
        "\n",
        "\n",
        "\n",
        "def test_one_epoch(model, data_loader, criterion):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    total_loss, total_acc =  0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_loader:\n",
        "            hat_y = model(X)\n",
        "            loss = criterion(hat_y.squeeze(), y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predicted = (hat_y.squeeze() > 0.5).float()\n",
        "            total_acc += (predicted == y).sum().item()/len(y)\n",
        "\n",
        "    return total_loss/len(data_loader), total_acc/len(data_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(model, train_loader, test_loader, criterion, optimizer, n_epoch):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch in range(param.n_epoch):\n",
        "\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {train_loss }, Acc: {train_acc} \")\n",
        "\n",
        "        test_loss, test_acc = test_one_epoch(model, test_loader, criterion)\n",
        "        print(f\"\\tValidation Loss: {test_loss }, Acc: {test_acc} \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "j9k06bjfcypD"
      },
      "outputs": [],
      "source": [
        "# --- Convert numpy.array to torch.tensor\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create TensorDatasets for train and test data\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Create DataLoaders for train and test data\n",
        "train_loader = DataLoader(train_dataset, batch_size=param.batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=param.batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlrDTuk5K65Q"
      },
      "source": [
        "# First model\n",
        "\n",
        "\n",
        "\n",
        "In the first model, you will successively\n",
        "- step-1) learn word embeddings $e^{<t>}$ of each item of the inut sequence $x^{(t)}$.\n",
        "    - This is done by learning an embedding matrix $E$. You will use the `nn.Embedding` layer in pytorch.\n",
        "    - In pytorch, the `nn.Embedding` layer does not really perform a matrix multiplication going from one-hot-encoding to embedding (it would be very costly to do that).\n",
        "    - In pytorch `nn.Embedding` is a special layer that goes directly from index-of-the-word-in-the-dictionary to the embedding $e^{(t)}$\n",
        "    - The embedding goes from `param.n_word` dimensions to  `param.n_embedding` dimensions\n",
        "- step-2) compute the average over time $t$ of the embedding $e^{(t)}$ obtained for each word $x^{(t)}$ of a sequence (you should use `torch.mean`)\n",
        "- step-3) apply a fully connected (`nn.linear` layer in pytorch) which output activation is a sigmoid (predicting the 0 or 1 rating)\n",
        "\n",
        "<img src=\"https://perso.telecom-paristech.fr/gpeeters/doc/Lab_DL_RNN_01.png\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zspaUptgtW9l",
        "outputId": "36af62da-a626-41cd-8c7e-c05e34eaace4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 100])\n",
            "torch.Size([64, 1])\n"
          ]
        }
      ],
      "source": [
        "if student:\n",
        "    # --- START CODE HERE (02)\n",
        "    class SimpleModel(nn.Module):\n",
        "        def __init__(self, param):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding(param.n_word, param.n_embedding)\n",
        "            self.fc = nn.Linear(param.n_embedding, param.n_out)\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "        def forward(self, x):\n",
        "            x = self.embedding(x)  # (batch, T_x, n_embedding)\n",
        "            x = torch.mean(x, dim=1)  # (batch, n_embedding)\n",
        "            x = self.fc(x)  # (batch, n_out)\n",
        "            x = self.sigmoid(x)\n",
        "            return x\n",
        "    # --- END CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "# --- Test\n",
        "torch.manual_seed(0)\n",
        "model = SimpleModel(param)\n",
        "print(X_train_tensor[:param.batch_size, :].size())\n",
        "print(model(X_train_tensor[:param.batch_size, :]).size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-RsBZDShCyag"
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(),param.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqQJLiu2c8_M",
        "outputId": "03d44d11-0694-49b7-b146-c5c46691e741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.6747531930503943, Acc: 0.618486253196931 \n",
            "\tValidation Loss: 0.6472773216569515, Acc: 0.6908967391304348 \n",
            "Epoch 2, Loss: 0.5944449674442905, Acc: 0.7352062020460358 \n",
            "\tValidation Loss: 0.5432630177501523, Acc: 0.7694213554987213 \n",
            "Epoch 3, Loss: 0.4897418047308617, Acc: 0.7975863171355498 \n",
            "\tValidation Loss: 0.45840003491972414, Acc: 0.8072250639386189 \n",
            "Epoch 4, Loss: 0.4190412707950758, Acc: 0.8285805626598466 \n",
            "\tValidation Loss: 0.4102556784744458, Acc: 0.8243286445012787 \n",
            "Epoch 5, Loss: 0.37674824790576533, Acc: 0.8445812020460358 \n",
            "\tValidation Loss: 0.38327097023844414, Acc: 0.8325207800511509 \n",
            "Epoch 6, Loss: 0.34965070167465895, Acc: 0.8558583759590793 \n",
            "\tValidation Loss: 0.367148749587481, Acc: 0.8377557544757033 \n",
            "Epoch 7, Loss: 0.3303366080117043, Acc: 0.8615968670076726 \n",
            "\tValidation Loss: 0.35657155262234874, Acc: 0.842391304347826 \n",
            "Epoch 8, Loss: 0.3154794965177546, Acc: 0.8678628516624042 \n",
            "\tValidation Loss: 0.34926891258305603, Acc: 0.8453884271099744 \n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "train(model, train_loader, test_loader, criterion, optimizer, param.n_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBqyzLJRUIsC"
      },
      "source": [
        "## Results\n",
        "\n",
        "After only 8 epochs, you should obtain an accuracy \"around\" 86.7%/ 84.5% for the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRP-h4Xr_HNJ"
      },
      "source": [
        "## Using the trained embedding to find equivalence between words\n",
        "\n",
        "Since the embedding is part of the models, we can look at the trained embedding matrix $E$ and use it to get the most similar words (according to the trained matrix $E$) in the dictionary.\n",
        "You will use the weights of the `nn.Embedding` layer to find the most similar words to `great`. We will use an Euclidean distance for that.\n",
        "\n",
        "- 1) Retrieve the weights of the `nn.Embedding` layer\n",
        "- 2) Get the position of `great` in the dictionary\n",
        "- 3) Knowing this position, get the word-embedding of `great`\n",
        "- 4) Find (using Euclidean distance), the closest embedded-words to `great`\n",
        "\n",
        "Remarks:\n",
        "- you can access a specific layer of the model by using the name you used to define `self.??? = nn.Embedding` in the `__init__`method of `SimpleModel`.\n",
        "- be careful about the order of the dimensions of the embedding matrix `E`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xMubRqJ_HNJ",
        "outputId": "424ae415-6c9e-4803-a22d-982a1c80ea45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index of 'great': 87\n",
            "Closest words to 'great':\n",
            "1: fun\n",
            "2: masterpiece\n",
            "3: excellent\n",
            "4: friendship\n",
            "5: definite\n",
            "6: perfect\n",
            "7: flawless\n",
            "8: recommended\n",
            "9: strong\n",
            "10: classic\n"
          ]
        }
      ],
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "if student:\n",
        "    # --- START CODE HERE (03)\n",
        "    # 1) Retrieve the weights of the nn.Embedding layer\n",
        "    E = model.embedding.weight.data.cpu().numpy()  # shape: (n_word, n_embedding)\n",
        "\n",
        "    # 2) Get the position of 'great' in the dictionary\n",
        "    idx_great = word_to_id.get('great', None)\n",
        "    print(\"Index of 'great':\", idx_great)\n",
        "\n",
        "    # 3) Get the word-embedding of 'great'\n",
        "    emb_great = E[idx_great]\n",
        "\n",
        "    # 4) Find the closest embedded-words to 'great' (Euclidean distance)\n",
        "    dists = cdist(E, emb_great.reshape(1, -1), metric='euclidean').squeeze()\n",
        "    # Exclude 'great' itself and get indices of the 10 closest words\n",
        "    closest_idx = np.argsort(dists)[1:11]\n",
        "    closest_words = [id_to_word.get(idx, f\"<UNK:{idx}>\") for idx in closest_idx]\n",
        "    print(\"Closest words to 'great':\")\n",
        "    for i, word in enumerate(closest_words):\n",
        "        print(f\"{i+1}: {word}\")\n",
        "    # --- END CODE HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK9e5Eo1Ks2a"
      },
      "source": [
        "# Second model\n",
        "\n",
        "In the second model, you will replace step-2 (which was \"compute the average over time $t$ of the embedding $e^{(t)}$\") by a RNN layer over time.\n",
        "More precisely you will use a LSTM (`nn.LSTM` layer in pytorch) with `param.n_lstm=100` units (or dimensions) in a Many-To-One configuration\n",
        "\n",
        "Don't forget that in ou data, the first dimension of `X_train/X_test` represents the batch (`batch_first=True` in pytorch).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dl-CSMKoViX",
        "outputId": "f0b0c23c-0ad8-4774-f6a7-c17c0fc4af0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 100])\n",
            "torch.Size([64, 1])\n"
          ]
        }
      ],
      "source": [
        "if student:\n",
        "    # --- START CODE HERE (04)\n",
        "    class LstmModel(nn.Module):\n",
        "        def __init__(self, param):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding(param.n_word, param.n_embedding)\n",
        "            self.lstm = nn.LSTM(param.n_embedding, param.n_lstm, batch_first=True)\n",
        "            self.fc = nn.Linear(param.n_lstm, param.n_out)\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "        def forward(self, x):\n",
        "            x = self.embedding(x)  # (batch, T_x, n_embedding)\n",
        "            x, _ = self.lstm(x)  # (batch, T_x, n_lstm)\n",
        "            x = x[:, -1, :]  # Take the last output of the LSTM (batch, n_lstm)\n",
        "            x = self.fc(x)  # (batch, n_out)\n",
        "            x = self.sigmoid(x)\n",
        "            return x\n",
        "    # --- END CODE HERE\n",
        "\n",
        "\n",
        "# --- Test\n",
        "torch.manual_seed(0)\n",
        "model = LstmModel(param)\n",
        "print(X_train_tensor[:param.batch_size, :].size())\n",
        "print(model(X_train_tensor[:param.batch_size, :]).size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-bp7PzX7oXtB",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), param.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP8TvdgNiUQd",
        "outputId": "b334662c-b318-49e5-ea53-19cb6f0d022f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.6336851448506651, Acc: 0.6224504475703325 \n",
            "\tValidation Loss: 0.5548016823008847, Acc: 0.7140984654731457 \n",
            "Epoch 2, Loss: 0.4833343529792698, Acc: 0.7674312659846547 \n",
            "\tValidation Loss: 0.43916834902275553, Acc: 0.7970348465473146 \n",
            "Epoch 3, Loss: 0.39423154523153137, Acc: 0.8238730818414322 \n",
            "\tValidation Loss: 0.4321940398353445, Acc: 0.8070891943734014 \n",
            "Epoch 4, Loss: 0.34830566730035845, Acc: 0.8493446291560103 \n",
            "\tValidation Loss: 0.37113789196514413, Acc: 0.8346067774936061 \n",
            "Epoch 5, Loss: 0.30522067713386875, Acc: 0.8714833759590793 \n",
            "\tValidation Loss: 0.37063817344510647, Acc: 0.8414641943734015 \n",
            "Epoch 6, Loss: 0.2729705990961446, Acc: 0.8882512787723784 \n",
            "\tValidation Loss: 0.3651688382448748, Acc: 0.8394820971867007 \n",
            "Epoch 7, Loss: 0.24405822723799045, Acc: 0.9023177749360614 \n",
            "\tValidation Loss: 0.3750662127190539, Acc: 0.8416240409207161 \n",
            "Epoch 8, Loss: 0.2220307461098027, Acc: 0.9121483375959079 \n",
            "\tValidation Loss: 0.4084377209548755, Acc: 0.8477861253196931 \n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "train(model, train_loader, test_loader, criterion, optimizer, param.n_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1LN_fjMWBHJ"
      },
      "source": [
        "## Results\n",
        "\n",
        "After only 8 epochs, you should obtain an accuracy around 91.1%/ 84.7% for the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qBjoRpoqv_g"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "To evaluate the work, you should rate the code for\n",
        "- 1) Data Pre-Processing (01)\n",
        "- 2) Simple model  (02)\n",
        "- 3) Find equivalence between words (03)\n",
        "- 4) LSTM model (04)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnwJcS93qv_g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
